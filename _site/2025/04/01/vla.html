<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>VLM/VLA相关调研 &#8211; siyu</title>
<meta name="description" content="一些VLM和VLA模型的简单介绍">
<meta name="keywords" content="act, diffusion, vla, deep learning, generative models, LAPA, flow matching">



<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="VLM/VLA相关调研">
<meta property="og:description" content="一些VLM和VLA模型的简单介绍">
<meta property="og:url" content="http://localhost:4000/2025/04/01/vla.html">
<meta property="og:site_name" content="siyu">





<link rel="canonical" href="http://localhost:4000/2025/04/01/vla.html">
<link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="siyu Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">
<!-- Webfonts -->
<link href="https://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script src="http://localhost:4000/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://localhost:4000/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://localhost:4000/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://localhost:4000/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://localhost:4000/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://localhost:4000/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://localhost:4000/images/apple-touch-icon-144x144-precomposed.png">



<!-- MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: [
      "MathMenu.js",
      "MathZoom.js",
      "AssistiveMML.js",
      "a11y/accessibility-menu.js"
    ],
    jax: ["input/TeX", "output/CommonHTML"],
    TeX: {
      extensions: [
        "AMSmath.js",
        "AMSsymbols.js",
        "noErrors.js",
        "noUndefined.js",
      ]
    }
  });
</script>

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
</head>

<body id="post" class="feature">

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->
<nav id="dl-menu" class="dl-menuwrapper" role="navigation">
	<button class="dl-trigger">Open Menu</button>
	<ul class="dl-menu">
		<li><a href="http://localhost:4000/">Home</a></li>
		
		<li>
			<a href="#">Posts</a>
			<ul class="dl-submenu">
				<li><a href="http://localhost:4000/posts/">All Posts</a></li>
				<li><a href="http://localhost:4000/tags/">All Tags</a></li>
			</ul>
		</li>
		
	</ul><!-- /.dl-menu -->
</nav><!-- /.dl-menuwrapper -->



<div class="entry-header">
  
  <div class="entry-image">
    <img src="http://localhost:4000/images/albumsbg.jpg" alt="VLM/VLA相关调研">
  </div><!-- /.entry-image -->
</div><!-- /.entry-header -->


<div id="main" role="main">
  <article class="hentry">

    <header class="header-title">
      <div class="header-title-wrap">
        
          <h1 class="entry-title"><a href="http://localhost:4000/2025/04/01/vla.html" rel="bookmark" title="VLM/VLA相关调研">VLM/VLA相关调研</a></h1>
        
        <h2><span class="entry-date date published"><time datetime="2025-04-01T00:00:00+08:00">April 01, 2025</time></span></h2>
        
        <p class="entry-reading-time">
          <i class="fa fa-clock-o"></i>
          
Reading time ~1 minute
        </p><!-- /.entry-reading-time -->
        
      </div><!-- /.header-title-wrap -->
    </header>

    <div class="entry-content">
      <h1 id="hv9Zu">VLM</h1>
<h2 id="qt9xg">Qwen系列</h2>
<h3 id="s5O1N">qwen-vl（[https://arxiv.org/abs/2308.12966](https://arxiv.org/abs/2308.12966)）</h3>
<p><img src="https://cdn.nlark.com/yuque/0/2025/png/50337259/1741597940578-9c18bcc3-a506-4192-80d2-019901d2c6de.png" alt="" /></p>

<p>qwen-vl的模型结构比较简单，图像处理模块来自clip的ViT-bigG，文本模块采用QwenLM作为主体，两者通过CrossAttn(adapter)进行桥接。adapter的结构会稍微复杂些<img src="https://cdn.nlark.com/yuque/0/2025/png/50337259/1741602327668-a1a31b95-7111-47f0-9ab3-4b94d25f287e.png" alt="" /></p>

<p>forward函数根据不同分辨率（支持两种分辨率）计算二维位置编码，分别合并到q和k上，其中q是可训练的参数，长度为256. kv来自vit的输出，最终通过attn映射到固定的256长度，再送入QwenLM进行后续计算。该模块是qwenvl能够分别以224<em>224分辨率和448</em>448分辨率运行的关键，因为无论是哪种分辨率，最终都映射为了256的长度，方便LLM进行处理。</p>

<p>训练时分成了三个阶段，第一阶段用大规模的网络爬虫获取的图文对，收集了5B图文数据，清洗后保留了1.4B，冻结LM层，采用无监督预训练任务。第二阶段采用多任务训练，caption ，vqa，grounding，ref grounding , Grounded Cap  , OCR  , Pure-text Autoregression，数据质量更高，图片分辨率更高，所有的参数都参与训练。第三阶段做SFT，冻结vit模块，主要训练指令跟随和对话能力，数据来源是模型生成+人工标注。</p>

<h3 id="h5D17"><font style="color:rgb(25, 27, 31);">Qwen2-VL（</font>[https://arxiv.org/abs/2409.12191](https://arxiv.org/abs/2409.12191)<font style="color:rgb(25, 27, 31);">）</font></h3>
<p><img src="https://cdn.nlark.com/yuque/0/2025/png/50337259/1741660391114-fb360ac4-1ae2-4442-bc8f-951252b43337.png" alt="" /></p>

<font style="color:rgb(25, 27, 31);">Qwen2-vl的模型结构略有改动，LM模块替换为Qwen2，改动较大的是visual encoder模块，vit从绝对位置编码改成2D-ROPE编码，放弃了cross attention改用MLP层，MLP会把2x2的token压缩为1个token，假设原始图像分辨率为224x224，patch size=14，那么vit输出的序列长度为16*16，经过MLP后变成8*8，加上两个special token（ &lt;|vision_start|&gt; and &lt;|vision_end|&gt;  ）最终变为66个token，送入LLM进行处理。 2x2的MLP比较特殊，像是一个特殊的卷积层，为了实现这个特殊的MLP，QWEN2VL提供了前置的图像预处理</font>
<p><strong><font style="color:rgb(31, 35, 40);background-color:rgb(246, 248, 250);">image_processing_qwen2_vl.py，</font></strong><img src="https://cdn.nlark.com/yuque/0/2025/png/50337259/1741664796991-30ae7683-cdf8-4c5c-9983-facfe796232b.png" alt="" /><font style="color:rgb(25, 27, 31);">会提前将图像切分为patch，然后合并成超大的patch，方便后续进行view操作。</font></p>

<font style="color:rgb(25, 27, 31);">第二个改进点是M-Rope，原始的qwenvl使用的是rope-1d，M-rope是rope-3d。更详细的解读可以看苏剑林的博客</font>
<p><a href="https://spaces.ac.cn/archives/10352/comment-page-1">https://spaces.ac.cn/archives/10352/comment-page-1</a></p>

<p><img src="https://cdn.nlark.com/yuque/0/2025/png/50337259/1742351961129-d12d292f-3e78-40ab-8780-e2051b98093c.png" alt="" /></p>

<p>第三点是qwen2-vl在训练数据中包含了图像和视频数据，视频保留每秒两帧，在attn之前采用深度为2的3d卷积进行压缩</p>

<p>训练方式和qwenvl一致，依旧采用三段式训练。</p>

<h3 id="CF1Lv">qwen2.5vl（[https://arxiv.org/pdf/2502.13923](https://arxiv.org/pdf/2502.13923)）</h3>
<p>除了lm模块替换为qwen2.5之外，在qwen2vl上的主要优化点：</p>

<p>1、视觉的encoder的一些小调整，例如加入swiglu，RMSnorm。比较大的调整是vit中的大多数attn层换成了 window-based attention。此处的window并不是滑动窗口，而是对图片按照112*112进行分块，然后在块内做attn计算</p>

<p>2、 dynamic FPS sampling和MRoPE  升级。在训练的过程中，对视频的fps进行动态采样，即支持多种fps，为了表示不同fps之间的差别，需嵌入视频的时间信息，因此将MRoPE的id与真实的视频时长对齐，这样就能表示每一帧大概属于原视频的哪个位置。<img src="https://cdn.nlark.com/yuque/0/2025/png/50337259/1741764100892-fc7476fa-5f07-4a03-8393-59fb5b0d6703.png" alt="" />可以参考这个例子，8秒的视频，每秒截取两帧，因此一共16张图片，如果采用0.5fps进行训练，那么共抽出4张图片送入vit，由于attn之前会用conv3d合并连续的两帧图片，所以最终只剩下2个图片对应的向量，即上图中的0和15. MRoPE是一个三维tuple，对图片或视频来说，是（时间戳，行坐标，列坐标）,对文本来说，是（位置，位置，位置）</p>

<p>3、高质量数据集的扩充。待补充</p>

<h2 id="hbtHs">internVL系列</h2>
<p>internVL是shanghai ai lab的工作，在各种榜单中排名都比较靠前。internVL1系列比较乱，从1.1到1.5，1.5之前的结构已经弃用，主要介绍1.5（<a href="https://arxiv.org/abs/2404.16821">https://arxiv.org/abs/2404.16821</a>）</p>

<p><img src="https://cdn.nlark.com/yuque/0/2025/png/50337259/1741830744833-1823c597-f99b-4d35-ae53-42417cec7be3.png" alt="" /></p>

<p>在图像的处理方面，internVL-1.5推理时(训练时并不支持)支持最高4K分辨率的图片，处理方式是将图片划分为不重叠的448*448的图像块，，并创建缩略图。每个子图会单独被VIT进行处理，vit处理后，假设输出维度<font style="color:rgb(25, 27, 31);">[BS, 32, 32, dim]，会将其改变成[BS, 32//2, 32//2, dim×(2^2)]，这样token数量就从32*32减少为16*16，减轻了后续LLM的压力。</font></p>

<font style="color:rgb(25, 27, 31);">MLP projector在论文里没有提及太多，但是看tensor的输出shape可以看出，mlp层会把每个token从12800压缩到6144，也就是和llm的hiddensize对齐，然后每张图片的token会直接concat</font>
<p><img src="https://cdn.nlark.com/yuque/0/2025/png/50337259/1741844099181-1a6ee90d-f6f8-4620-8327-ec0d66455a37.png" alt="" /></p>

<p>训练是两阶段的，先训练vit和MLP，再做全参数训练</p>

<h3 id="Gym22">internvl-2（[https://internvl.github.io/blog/2024-07-02-InternVL-2.0/](https://internvl.github.io/blog/2024-07-02-InternVL-2.0/)）</h3>
<p>internvl 2 没有论文，模型结构与intervl 1.5保持一致</p>

<p>主要是优化了训练策略（但是没有说是怎么做的，在2.5vl中提及了做法），额外支持视频和医学图像作为输入，以及支持多种输出，例如输出图像等。但实际上并不是通过internvl2端到端的输出，而是额外接入一些下游decoder。训练时第一阶段只训练mlp层。</p>

<h3 id="hfZlw">internvl-2.5（[https://arxiv.org/abs/2412.05271](https://arxiv.org/abs/2412.05271)）</h3>
<p>internvl-2.5的模型结构与之前依旧保持一致。在训练阶段额外增加了一个阶段，第一阶段训练MLP projector，第二阶段是vit+MLP，第三阶段是全参数。</p>

<p><img src="https://cdn.nlark.com/yuque/0/2025/png/50337259/1741856251476-2c7a3f5a-bd2f-413d-8a88-591c0e7b0e7c.png" alt="" /></p>

<p>vl2.5提出了一种训练方式叫做<strong><font style="color:rgb(0, 0, 0);">progressive scaling strategy，</font></strong><font style="color:rgb(0, 0, 0);">实际做法是先训练较小版本的模型，比如说vit-6b+20B llm，再训练vit-6b+72B llm，那么在训练vit-6b+72B llm时，就可以在stage1的训练时使用vit-6b+20B llm在stage1.5训练后的vit作为vit-6b+72B llm的视觉模块启动模型，此时可以跳过stage1.5阶段，减少了训练成本。</font></p>

<font style="color:rgb(0, 0, 0);">vl2.5在数据层面做了很多工作，待补充。</font>

<h2 id="mI0yv"><font style="color:rgb(0, 0, 0);">Gemini</font></h2>
<p>技术报告开源的内容不多，唯一的架构相关图如下</p>

<p><img src="https://cdn.nlark.com/yuque/0/2025/png/50337259/1742206062669-0d4d6720-829c-4ec5-a3e2-38e4555c4300.png" alt="" /></p>

<p>主流的猜测认为Gemini在图像侧使用的是VQVAE等方式，将图片映射为一些离散的token，而模型输出的token也可以被decode成一张图片，从而实现图片的输入和输出。</p>

<h1 id="BTdvn">VLA</h1>
<h2 id="hUky1">ACT（<font style="color:rgb(25, 27, 31);">Action Chunking with Transformers</font>）系列：</h2>
<font style="color:rgb(0, 0, 0);">ALOHA（</font>
<p><a href="https://tonyzhaozh.github.io/aloha/">https://tonyzhaozh.github.io/aloha/</a><font style="color:rgb(0, 0, 0);">）、</font><font style="color:rgb(25, 27, 31);">Mobile ALOHA（</font><a href="https://mobile-aloha.github.io/cn.html">https://mobile-aloha.github.io/cn.html</a><font style="color:rgb(25, 27, 31);">）、ALOHA 2（</font><a href="https://aloha-2.github.io/">https://aloha-2.github.io/</a><font style="color:rgb(25, 27, 31);">）</font></p>

<font style="color:rgb(25, 27, 31);">ALOHA是斯坦福2023年推出的一份工作</font>
<p><strong><font style="color:rgb(77, 77, 77);">A</font></strong><font style="color:rgb(77, 77, 77);"> </font><strong><font style="color:rgb(77, 77, 77);">L</font></strong><font style="color:rgb(77, 77, 77);">ow-cost </font><strong><font style="color:rgb(77, 77, 77);">O</font></strong><font style="color:rgb(77, 77, 77);">pen-source </font><strong><font style="color:rgb(77, 77, 77);">Ha</font></strong><font style="color:rgb(77, 77, 77);">rdware System for Bimanual Teleoperation</font><font style="color:rgb(25, 27, 31);">，24年在前作的基础上继续推出了Mobile ALOHA，项目从硬件到软件完全开源。ALOHA系列采用transformer架构，采用模仿学习的方式，为了解决模仿学习中的误差的累积问题，提出了ACT（</font><font style="color:rgb(0, 0, 0);">Action Chunking with Transformers</font><font style="color:rgb(25, 27, 31);">）算法。</font></p>

<font style="color:rgb(25, 27, 31);"> </font>
<p><img src="https://cdn.nlark.com/yuque/0/2025/png/50337259/1741585904331-5cb6ff33-864f-4c69-97b0-8db006234bc9.png" alt="" /></p>

<font style="color:rgb(25, 27, 31);">解决累计误差的方案是每次预测未来k步的action而不是只预测下一步，并且为了保证动作的连贯性（如果每k步才给model一个动作的observation，模型输出的动作可能会不连贯），在每一步都会预测未来k步的动作，而当前时间步最终采用什么样的action，是历史k步对当前步预测的加权求和（相当于从一个马氏链变成了一个非马氏链）。训练和推理算法如下：</font>

<p><img src="https://cdn.nlark.com/yuque/0/2025/png/50337259/1741586424076-ba5e6ef9-a088-4cfc-93e9-dc2f487a53ba.png" alt="" /></p>

<p>模型结构比较简单</p>

<p><img src="https://cdn.nlark.com/yuque/0/2025/png/50337259/1741573251858-16cb1148-5f08-4364-8dfc-57360b9f1fc1.png" alt="" /></p>

<font style="color:rgb(25, 27, 31);">如上图所示，ACT的整体结构类似CVAE模型。左端为z的编码器，通过bert对当前关节位置+未来k步动作序列+posemb进行编码，得到z。右侧为该模型的解码器，transformer架构，蓝色的encoder接收4个摄像头的Emd，关节，和隐变量z，decoder输出未来k步动作序列。损失函数与CVAE基本保持一致，框架可以参考下述代码中的transformer和encoder。</font>

<p><img src="https://cdn.nlark.com/yuque/0/2025/png/50337259/1741574222685-06bb8b6a-4812-4e4d-a367-5704f5212a25.png" alt="" /></p>

<font style="color:rgb(25, 27, 31);">Detrvae的forward代码如下：</font>

<p><img src="https://cdn.nlark.com/yuque/0/2025/png/50337259/1741575224979-4df793a3-11fc-4ad7-b52b-7ac00dfc0412.png" alt="" /></p>

<p>训练阶段通过encoder生成vae的mu和var，重参数化后映射为一个z。非训练模式直接从0值向量做映射</p>

<p><img src="https://cdn.nlark.com/yuque/0/2025/png/50337259/1741578827571-67068d13-8359-4c37-9e57-c9ade65f504d.png" alt="" /></p>

<p>得到latent input（即z）后，会处理图片，position和关节信息，送入transformer进行处理，transformer结果会经过head分别映射为action和is_pad_hat（疑似废弃变量，未参与loss计算，可能之前用于预测pad token，后面废除了）.</p>

<p>self.transformer中encoder结构与bert一致，decoder结构较为复杂，添加了learnable query作为Q。</p>

<p><img src="https://cdn.nlark.com/yuque/0/2025/png/50337259/1741589490315-35e56cf2-2817-4bbd-baae-d247f2523cf1.png" alt="" /></p>

<p>模型整体参数只有80M，infer time 0.01s</p>

<font style="color:rgb(25, 27, 31);">Mobile ALOHA</font>

<font style="color:rgb(25, 27, 31);">ALOHA是机械臂，Mobile ALOHA在ALOHA的基础上添加了轮足，模型结构上并没有差异。主要讲述硬件系统，数据构建，任务设置以及实验结果。为了适配数据集，训练方式做了一定调整，相当于多任务学习，也发现了 positive transfer  现象。</font>

<font style="color:rgb(25, 27, 31);">ALOHA 2</font>

<font style="color:rgb(25, 27, 31);">主要是硬件和</font>
<font style="color:rgba(0, 0, 0, 0.85);">MuJoCo 模型的改进，mujoco是一个物理引擎，常用于仿真训练</font>

<h2 id="fep17">RT系列</h2>
<font style="color:rgb(25, 27, 31);">RT系列模型是deepmind发表的机器人相关工作，RT-1发布于2022年，RT-2发布于2023年。RT-1（</font>
<p><a href="https://arxiv.org/abs/2212.06817">https://arxiv.org/abs/2212.06817</a><font style="color:rgb(25, 27, 31);">）模型结构是基于transformer做的改动，主要的发力点是大规模多任务训练（130k数据，700多个任务），依旧是采用行为克隆进行训练。RT-1采用EfficientNet做图像处理，USE做文字处理，通过FiLM模块对两个模态信息进行融合。通过 TokenLearner 模块（一个注意力模块）对token进行选择，最终送入transformer结构输出最终的action。</font></p>

<p><img src="https://cdn.nlark.com/yuque/0/2025/png/50337259/1742196446806-7bf3767d-c435-42ba-9591-b637946711b9.png" alt="" /></p>

<p>RT-2（<a href="https://arxiv.org/abs/2307.15818">https://arxiv.org/abs/2307.15818</a>）</p>

<p>VLA这个名词最早即在rt-2的文章中提出。虽然rt-1使用了130k数据，但训练出来的模型通用性依旧受限，而短时间内收集数亿的训练数据也不太可能，因此deepmind想要借助已经训练好的VLM，用机器人领域数据做co-fine-tuning，达到端到端的控制以及常识的理解能力。<img src="https://cdn.nlark.com/yuque/0/2025/png/50337259/1742197923338-9c3fca49-bff0-4435-9511-ead593f12dff.png" alt="" /></p>

<p>RT-2使用的VLM是google的Pali-X和PaLM-E。Pali-X可以看作是Gemini的雏形,PaLM-E也是google的工作，本身就是具身模型，只不过不是直接端到端输出action，而是输出高阶的指令，例如： First grasp yellow block and place it on the table, then grasp the blue block. RT-2直接输出关节的控制命令，8个自由度，每个自由度被离散为256个值，最终一个action可以被8个整数表示，这些token可以采用原来的数字token。训练数据不是只包含了机器人数据，而是结合了来自互联网的原始数据，这样使得策略的泛化性更强。</p>

<p>RT-2释放的最大版本是55B，显然不可能直接部署在机器人上做闭环控制，最终的解决方案是把模型部署在TPU云上，最终实现了1-3hz的控制频率。</p>

<p>数据集构成：<font style="color:rgb(25, 27, 31);">PaLI和Palm-e训练数据：VQA，Caption等，机器人数据来自RT1，控制训练时的采样比例使得机器人数据占50%（或66%）</font></p>

<h2 id="a7hV6">Open VLA（[https://openvla.github.io/](https://openvla.github.io/)）</h2>
<p><img src="https://cdn.nlark.com/yuque/0/2025/png/50337259/1742192958915-5df62cb1-4f51-44fd-bf0c-e7259f288bb2.png" alt="" /></p>

<p>7B级别的模型，训练数据970K真实世界的机器人数据。主框架是Llama2，视觉模块是vit。该工作的契机与RT-2相同：<font style="color:rgb(25, 27, 31);">机器人数据集规模远不及互联网数据集，因此采用已有的大模型进行继续训练。</font></p>

<p><img src="https://cdn.nlark.com/yuque/0/2025/png/50337259/1742193341031-da8d5a63-519b-4f24-86e1-59e985210a33.png" alt="" /></p>

<p>64张A100训练14天，推理时采用bf16单卡4090，控制频率在6hz左右</p>

<h2 id="DkPqc">Diffusion Policy （[https://arxiv.org/abs/2303.04137](https://arxiv.org/abs/2303.04137)）</h2>
<p>材料<a href="https://zhuanlan.zhihu.com/p/670555655">https://zhuanlan.zhihu.com/p/670555655</a></p>

<p><img src="https://cdn.nlark.com/yuque/0/2025/png/50337259/1742201607541-3c496cc8-b01d-4f18-846e-4222c3731bdd.png" alt="" /></p>

<p>传统的有监督算法难以解决多峰分布、序列相关性问题。MSE损失函数无法解决多峰问题，而离散化则很难做多步预测，因为高维空间算力成本高且过于离散。为了解决这些问题，作者尝试将DDPM引入到机器人控制领域。</p>

<p><img src="https://cdn.nlark.com/yuque/0/2025/png/50337259/1742204117557-1b079283-0f58-4235-8047-ce80cca57a13.png" alt="" /></p>

<p><img src="https://cdn.nlark.com/yuque/0/2025/png/50337259/1742203936144-a4db84fe-840e-4939-87ca-2da2e5532f29.png" alt="" /></p>

<p>优势：训练稳定，操作稳定</p>

<p>3080显卡推理，0.1s 推理延迟</p>

<h2 id="RMqzr">Pi0（[https://arxiv.org/abs/2410.24164](https://arxiv.org/abs/2410.24164)）</h2>
<p>文章主张使用大量的数据预训练机器人，包括各种任务，各种型号机器人甚至非机器人数据，然后再进行下游微调，但是目前缺少大量数据，不确定合适的模型结构，以及不知道训练方式。因此提出了一种原型模型以及对应的训练框架，即Pi0。模型结构如下所示</p>

<p><img src="https://cdn.nlark.com/yuque/0/2025/png/50337259/1742281734602-cc8c6e4c-ba1b-4030-ba4b-79f2dde786dc.png" alt="" /></p>

<p>提出的训练框架即pre-training+post-training。最终开源的模型是在超过1w小时的机器人数据上进行pretrain，并在一系列下游任务上进行微调（例如折叠衣服，擦桌子等）。模型的控制频率在50hz左右。</p>

<p><img src="https://cdn.nlark.com/yuque/0/2025/png/50337259/1742282629901-035a1bc6-a93a-403a-b68a-3ee397394700.png" alt="" /></p>

<p>模型的输入包含图片，语言，关节角向量。图片和关节向量会经过encoder后被线性层投影为tokens。action expert采用流匹配损失进行训练，类似于扩散模型也是去噪模型</p>

<p>在官方的实现中，action expert也是一个gemma模型，action expert和pre-trained VLM在attn层会进行交互，而其他层都是独立的，在前向推理时，pre-trained VLM的结果会被kv cache储存，action expert会在去噪过程中多次运行，为了加速这个过程，因此action expert减少了FFN层的参数。</p>

<p><img src="https://cdn.nlark.com/yuque/0/2025/png/50337259/1742287170145-6b98bee8-6aa5-46b6-8f3b-74be291109b4.png" alt="" /></p>

<h2 id="WnOik">Gemini-ROBOTICS（[https://storage.googleapis.com/deepmind-media/gemini-robotics/gemini_robotics_report.pdf](https://storage.googleapis.com/deepmind-media/gemini-robotics/gemini_robotics_report.pdf)）</h2>
<p><a href="https://www.youtube.com/watch?v=4MvGnmmP3c0">https://www.youtube.com/watch?v=4MvGnmmP3c0</a></p>

<font style="color:rgb(25, 27, 31);">Gemini Robotics系列推出了两款模型，Gemini Robotics-ER和Gemini Robotics，两个模型都是基于Gemini 2.0架构构建，但并未放出技术细节。Gemini Robotics-ER是一个具身推理的vlm，增强了目标检测，轨迹预测等任务，还可以zero-shot生成机器人执行代码。</font>

<p><img src="https://cdn.nlark.com/yuque/0/2025/png/50337259/1742276864669-0cb56e68-8bfe-4f8f-98d5-096ab7696e99.png" alt="" /></p>

<p><img src="https://cdn.nlark.com/yuque/0/2025/png/50337259/1742277527107-119da1ed-9b5d-44f1-8d8b-a0329595eef4.png" alt="" /></p>

<font style="color:rgb(25, 27, 31);">Gemini Robotics是基于Gemini Robotics-ER进行finetune的VLA，可以直接控制机器人的关节。 Gemini Robotics-ER  这类的模型延迟较高，硬件需求较大。为了解决这些问题Gemini Robotics拆解成了两部分，云端部署VLA backbone，机器端部署 Gemini Robotics decoder  。云端部署的是Gemini Robotics-ER的蒸馏版本，延迟从秒级优化到160ms。云端加机器端一起可以做到250ms延迟，采用action chunk最终可以实现50hz控制频率。</font>

<font style="color:rgb(25, 27, 31);"> </font>
<p><img src="https://cdn.nlark.com/yuque/0/2025/png/50337259/1742278394758-061fc3e3-0563-4513-b25e-b9337932f44e.png" alt="" /></p>

<h2 id="Pa0xM">GR00T([https://d1qx31qr3h6wln.cloudfront.net/publications/GR00T_1_Whitepaper.pdf](https://d1qx31qr3h6wln.cloudfront.net/publications/GR00T_1_Whitepaper.pdf))</h2>
<p><img src="https://cdn.nlark.com/yuque/0/2025/png/50337259/1742356570058-4f8857c6-c018-427c-9403-97398e52798e.png" alt="" /></p>

<p>VLM+流匹配。模型在训练的时候采用了LAPA的技术，使得可以从人类或机器人的视频中学习动作。</p>

<p>LAPA是一种逆运动学模型，思想是先用间隔时间T的两帧视频训练vq-vae模型，编码时采用x和x_T作为输入，输出隐变量z，解码时输入x和z，输出x_T.  隐变量z会作为潜在动作标签，训练vlm+latent action head基于x预测z，最终会用一部分真实的动作数据集，训练vlm+action head预测真实的动作。</p>

<p><img src="https://cdn.nlark.com/yuque/0/2025/png/50337259/1742366000647-fc5cb3a4-a665-413e-a1ef-66357f8e1984.png" alt="" /></p>

<font style="color:rgb(25, 27, 31);"></font>

<font style="color:rgb(25, 27, 31);"></font>

<font style="color:rgb(25, 27, 31);"></font>

<font style="color:rgb(25, 27, 31);"></font>

<font style="color:rgb(25, 27, 31);"></font>

<font style="color:rgb(25, 27, 31);"></font>

<font style="color:rgb(25, 27, 31);"></font>

<font style="color:rgb(25, 27, 31);"></font>

<font style="color:rgb(25, 27, 31);"></font>

<font style="color:rgb(25, 27, 31);"></font>

<font style="color:rgb(25, 27, 31);"></font>


      <footer class="entry-meta">
        <span class="entry-tags"><a href="http://localhost:4000/tags/#act" title="Pages tagged act" class="tag"><span class="term">act</span></a><a href="http://localhost:4000/tags/#diffusion" title="Pages tagged diffusion" class="tag"><span class="term">diffusion</span></a><a href="http://localhost:4000/tags/#vla" title="Pages tagged vla" class="tag"><span class="term">vla</span></a><a href="http://localhost:4000/tags/#deep learning" title="Pages tagged deep learning" class="tag"><span class="term">deep learning</span></a><a href="http://localhost:4000/tags/#generative models" title="Pages tagged generative models" class="tag"><span class="term">generative models</span></a><a href="http://localhost:4000/tags/#LAPA" title="Pages tagged LAPA" class="tag"><span class="term">LAPA</span></a><a href="http://localhost:4000/tags/#flow matching" title="Pages tagged flow matching" class="tag"><span class="term">flow matching</span></a></span>
        
        
      </footer>
    </div><!-- /.entry-content -->

    <div class="read-more">
  
    <div class="read-more-header">
      <a href="http://localhost:4000/2022/06/06/imagen.html" class="read-more-btn">Read More</a>
    </div><!-- /.read-more-header -->
    <div class="read-more-content">
      <h3><a href="http://localhost:4000/2022/06/06/imagen.html" title="深入理解 Imagen">深入理解 Imagen</a></h3>
      <p>关于 Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding 的探讨 <a href="http://localhost:4000/2022/06/06/imagen.html">Continue reading</a></p>
    </div><!-- /.read-more-content -->
  
  <div class="read-more-list">
    
  </div><!-- /.read-more-list -->
</div><!-- /.read-more -->
  </article>
</div><!-- /#main -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="http://localhost:4000/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="http://localhost:4000/assets/js/scripts.min.js"></script>


<!-- Asynchronous Google Analytics snippet -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'G-2WZMKC0BR6', 'auto');  
  ga('require', 'linkid', 'linkid.js');
  ga('send', 'pageview');
</script>


<!-- Link Gitalk 的支持文件  -->
<link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<script src="https://unpkg.com/gitalk@latest/dist/gitalk.min.js"></script> 
<div id="gitalk-container"></div>     <script type="text/javascript">
    var gitalk = new Gitalk({

    // gitalk的主要参数
        clientID: '7c854c0f32253f034752',
        clientSecret: '090c39c28b126c78184f1b2fd4d12f30d18d196f',
        repo: 'sunlin-ai.github.io',
        owner: 'sunlin-ai',
        admin: ['sunlin-ai'],
        id:decodeURI(window.location.pathname),

    });
    gitalk.render('gitalk-container');
</script> 
<!-- Gitalk end -->

<div class="footer-wrapper">
  <footer role="contentinfo">
    <span>&copy; 2025 SI-YU. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> using the <a href="https://mademistakes.com/work/hpstr-jekyll-theme/" rel="nofollow">HPSTR Theme</a>.</span>
  </footer>
</div><!-- /.footer-wrapper -->

	        

</body>
</html>
