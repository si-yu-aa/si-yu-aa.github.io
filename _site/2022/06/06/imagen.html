<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>深入理解 Imagen &#8211; siyu</title>
<meta name="description" content="关于 Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding 的探讨">
<meta name="keywords" content="imagen, diffusion, score function, guidance, deep learning, generative models">



<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="深入理解 Imagen">
<meta property="og:description" content="关于 Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding 的探讨">
<meta property="og:url" content="http://localhost:4000/2022/06/06/imagen.html">
<meta property="og:site_name" content="siyu">





<link rel="canonical" href="http://localhost:4000/2022/06/06/imagen.html">
<link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="siyu Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">
<!-- Webfonts -->
<link href="https://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script src="http://localhost:4000/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://localhost:4000/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://localhost:4000/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://localhost:4000/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://localhost:4000/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://localhost:4000/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://localhost:4000/images/apple-touch-icon-144x144-precomposed.png">



<!-- MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: [
      "MathMenu.js",
      "MathZoom.js",
      "AssistiveMML.js",
      "a11y/accessibility-menu.js"
    ],
    jax: ["input/TeX", "output/CommonHTML"],
    TeX: {
      extensions: [
        "AMSmath.js",
        "AMSsymbols.js",
        "noErrors.js",
        "noUndefined.js",
      ]
    }
  });
</script>

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
</head>

<body id="post" class="feature">

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->
<nav id="dl-menu" class="dl-menuwrapper" role="navigation">
	<button class="dl-trigger">Open Menu</button>
	<ul class="dl-menu">
		<li><a href="http://localhost:4000/">Home</a></li>
		
		<li>
			<a href="#">Posts</a>
			<ul class="dl-submenu">
				<li><a href="http://localhost:4000/posts/">All Posts</a></li>
				<li><a href="http://localhost:4000/tags/">All Tags</a></li>
			</ul>
		</li>
		
	</ul><!-- /.dl-menu -->
</nav><!-- /.dl-menuwrapper -->



<div class="entry-header">
  
  <div class="entry-image">
    <img src="http://localhost:4000/images/albumsbg.jpg" alt="深入理解 Imagen">
  </div><!-- /.entry-image -->
</div><!-- /.entry-header -->


<div id="main" role="main">
  <article class="hentry">

    <header class="header-title">
      <div class="header-title-wrap">
        
          <h1 class="entry-title"><a href="http://localhost:4000/2022/06/06/imagen.html" rel="bookmark" title="深入理解 Imagen">深入理解 Imagen</a></h1>
        
        <h2><span class="entry-date date published"><time datetime="2022-06-06T00:00:00+08:00">June 06, 2022</time></span></h2>
        
        <p class="entry-reading-time">
          <i class="fa fa-clock-o"></i>
          
Reading time ~4 minutes
        </p><!-- /.entry-reading-time -->
        
      </div><!-- /.header-title-wrap -->
    </header>

    <div class="entry-content">
      <p>Google Brain 推出的 Imagen<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>，再一次突破人类想象力，将文本生成图像的逼真度和语言理解提高到了前所未有的新高度！比前段时间 OpeAI 的 DALL·E 2 <sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>更强！</p>

<h2 id="imagen">Imagen</h2>

<p>Imagen 的组成：(1) 文本编码器，用于将文本映射成 embeddings 序列 ；(2) 级联条件扩散模型，用于将 embeddings 序列映射成图像，并逐步增加图像分辨率（参见 图 A.4)，以下将详细描述这些组件。</p>

<figure align="center">
  <img src="/images/image-20220606210644557.png" style="zoom:100%" />
</figure>
<div>
    <center style="color:#C0C0C0;text-decoration:underline;font-size: 15px">
        图1: Imagen 网络架构
    </center>
    <br />
</div>

<h3 id="文本编码器">文本编码器</h3>

<p>文本图像模型需要强大的语义文本编码器来捕获任意自然语言文本输入的复杂性和组合性。</p>

<p>在当前的文本到图像模型中通常使用在配对的图文数据集上训练的文本编码器，比如 CLIP。大型的语言模型可以作为另一种选择，例如 BERT、 GPT、 T5 ， 语言模型在纯文本语料库上训练，训练数据远多于成对的图像-文本数据， 所以其可以接触更加丰富和广泛分布的文本。语言模型通常也大得多（例如，PaLM 有 540 B 参数，而 CoCa  有1B 参数）。</p>

<p>Imagen 研究比较了预训练文本编码器：BERT 、T5 和 CLIP。这些文本编码器的权重是冻结的，这样做的好处是可以离线计算文本嵌入，在训练文本到图像生成模型期间，计算或内存占用可以忽略不计。经过实验比较发现文本编码器大小会影响文本到图像生成质量， T5-XXL 在图像-文本对齐、图像逼真度方面可以取得最好的成绩。</p>

<figure align="center">
  <img src="/images/image-20220606211035406.png" style="zoom:100%" />
</figure>
<div>
    <center style="color:#C0C0C0;text-decoration:underline;font-size: 15px">
        图2: 不同文本编码器训练性能比较
    </center>
    <br />
</div>

<figure align="center">
  <img src="/images/image-20220606211106144.png" style="zoom:100%" />
</figure>
<div>
    <center style="color:#C0C0C0;text-decoration:underline;font-size: 15px">
        图3: 不同文本编码器训练收敛比较
    </center>
    <br />
</div>

<h3 id="无分类器指导的扩散模型">无分类器指导的扩散模型</h3>

<p><strong>（1）扩散过程</strong></p>

<p>\(\mathbf{z}_{t}=\alpha_{t} \mathbf{x}+\sigma_{t} \boldsymbol{\epsilon_1}\)，其中 \(\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\)。</p>

<p>根据正向过程的定义可得：\(\mathbf{z}_s=\alpha_s \mathbf{x}+\sigma_s \boldsymbol{\epsilon_2}\) ，则 \(\mathbf{x}=\frac{1}{\alpha_s}(\mathbf{z}_s-\sigma_s \boldsymbol{\epsilon_2})\)，将 \(\mathbf{x}\) 代入上式得：</p>

\[\mathbf{z}_{t}=\frac{\alpha_{t}}{\alpha_s}(\mathbf{z}_s-\sigma_s \boldsymbol{\epsilon}_2)+\sigma_{t} \boldsymbol{\epsilon_1}=\frac{\alpha_{t}}{\alpha_s}\mathbf{z}_s-\frac{\alpha_{t}\cdot\sigma_s}{\alpha_s}\boldsymbol{\epsilon}_2+\sigma_{t} \boldsymbol{\epsilon_1}=\frac{\alpha_{t}}{\alpha_s}\mathbf{z}_s+\sqrt{\sigma_{t}^2-\frac{\alpha_t^2\cdot\sigma_s^2}{\alpha_s^2}}\cdot\epsilon\]

<p><small>(上式中，两个正太分布相减，其方差应该是相加，但是根据论文的推导方差为相减，这里是个疑问 )</small></p>

<p>定义：\(\sigma_{t \mid s}^{2}=\left(1-e^{\lambda_{t}-\lambda_{s}}\right) \sigma_{t}^{2}\) ，其中 \(\lambda_{t}=\log \left[\alpha_{t}^{2} / \sigma_{t}^{2}\right]\)，则：</p>

\[\mathbf{z}_{t}=\left(\alpha_{t} / \alpha_{s}\right) \mathbf{z}_{s}+\sqrt{\sigma_{t \mid s}^{2}}\cdot\epsilon\]

<p>所以：</p>

\[q\left(\mathbf{z}_{t} \mid \mathbf{x}\right)=\mathcal{N}\left(\mathbf{z}_{t} ; \alpha_{t} \mathbf{x}, \sigma_{t}^{2} \mathbf{I}\right), \quad q\left(\mathbf{z}_{t} \mid \mathbf{z}_{s}\right)=\mathcal{N}\left(\mathbf{z}_{t} ;\left(\alpha_{t} / \alpha_{s}\right) \mathbf{z}_{s}, \sigma_{t \mid s}^{2} \mathbf{I}\right)\]

<p>式中， \(0 \leq s&lt;t \leq 1, \sigma_{t \mid s}^{2}=\left(1-e^{\lambda_{t}-\lambda_{s}}\right) \sigma_{t}^{2}\), \(\alpha_{t}, \sigma_{t}\) 表示可微噪声schedule， \(\lambda_{t}=\log \left[\alpha_{t}^{2} / \sigma_{t}^{2}\right]\) 表示信噪比，其随着时间 \(t\) 逐步降低，直到 \(q\left(\mathbf{z}_{1}\right) \approx \mathcal{N}(\mathbf{0}, \mathbf{I})\)。</p>

<p><strong>（2）生成过程</strong></p>

<p>反转上述扩散过程，可以看作学习解噪 \(\mathbf{z}_{t} \sim q\left(\mathbf{z}_{t} \mid \mathbf{x}\right)\) ，即根据 \(\mathbf{z}_{t}\) 和其他条件估计 \(\mathbf{x}\) ，可以表示成：</p>

\[\hat{\mathbf{x}}_{\theta}\left(\mathbf{z}_{t}, \lambda_{t}, \mathbf{c}\right) \approx \mathbf{x}\]

<p>式中， \(\mathbf{c}\) 为可选的条件信息（比如文本embeddings 或者低分辨率图像）。训练 \(\hat{\mathbf{x}}_{\theta}\) 使用加权 MSE 损失函数：</p>

\[\mathbb{E}_{\boldsymbol{\epsilon}, t}\left[w\left(\lambda_{t}\right)\left\|\hat{\mathbf{x}}_{\theta}\left(\mathbf{z}_{t}, \lambda_{t}, \mathbf{c}\right)-\mathbf{x}\right\|_{2}^{2}\right]\]

<p>式中， \((\mathbf{x}, \mathbf{c})\) 是 数据-条件对, \(t \sim \mathcal{U}([0,1]), \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\),  \(\alpha_{t}, \sigma_{t}, w_{t}\) 是关于 \(t\) 的函数，其影响样本生成质量。</p>

<p>因为 \(\mathbf{z}_{t}=\alpha_{t} \mathbf{x}+\sigma_{t} \boldsymbol{\epsilon}\)，对 \(\epsilon\) 参数化，则 \(\hat{\mathbf{x}}_{\theta}\left(\mathbf{z}_{t}, \lambda_{t}, \mathbf{c}\right)=\left(\mathbf{z}_{t}-\sigma_{t} \epsilon_{\theta}\left(\mathbf{z}_{t}, \lambda_{t}, \mathbf{c}\right)\right) / \alpha_{t}\) ，所以损失函数可以简化为：</p>

\[\mathbb{E}_{\boldsymbol{\epsilon}, t}\left[w\left(\lambda_{t}\right)\left\|\epsilon_{\theta}\left(\mathbf{z}_{t}, \lambda_{t}, \mathbf{c}\right)-\epsilon\right\|_{2}^{2}\right]\]

<p>训练模型时，使用 Classifier-free guidance 方法，在单个扩散模型上同时训练无条件和带条件目标，具体做法是在训练模型时随机（一般以 \(10 \%\) 的概率）丢弃 \(\mathbf{c}\)，得到 \(\epsilon_{\theta}(\mathbf{z}_{t},\lambda_{t},\mathbf{c})\) 之后，使用下式更新：</p>

\[\tilde{\boldsymbol{\epsilon}}_{\theta}\left(\mathbf{z}_{t}, \lambda_{t}, \mathbf{c}\right)=w \boldsymbol{\epsilon}_{\theta}\left(\mathbf{z}_{t}, \lambda_{t}, \mathbf{c}\right)+(1-w) \boldsymbol{\epsilon}_{\theta}\left(\mathbf{z}_{t}, \lambda_{t}\right)\]

<p>式中, \(\boldsymbol{\epsilon}_{\theta}\left(\mathbf{z}_{t}, \lambda_{t}, \mathbf{c}\right)\) 和 \(\boldsymbol{\epsilon}_{\theta}\left(\mathbf{z}_{t}, \lambda_{t}\right)\) 分贝是带条件和无条件的 \(\boldsymbol{\epsilon}\) 预测值， \(w\) 是指导权重，当\(w=1\) 时，抑制了 classifier-free guidance, 当 \(w&gt;1\) 会增强 guidance 的影响。</p>

<p>进一步可以估计 score： \(\tilde{\boldsymbol{\epsilon}}_{\theta}\left(\mathbf{z}_{t}, \lambda_{t}, \mathbf{c}\right) \approx-\sigma_{t} \nabla_{\mathbf{z}_{t}} \log p\left(\mathbf{z}_{t} \mid \mathbf{c}\right)\)，其中 \(p\left(\mathbf{z}_{t} \mid \mathbf{c}\right)\) 是以 \(\mathbf{c}\) 为条件关于 \(\mathbf{z}_{t}\) 的概率密度。</p>

<p>为了从扩散模型中采样，可以从随机噪声 \(\mathbf{z}_{1} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\) 开始，然后使用 DDIM 方法采样，具体过程为：</p>

<ul>
  <li>
    <p>step1： 由模型得到 \(\epsilon_{\theta}\left(\mathbf{z}_{t}, \lambda_{t}, \mathbf{c}\right)\) 和  \(\epsilon_{\theta}\left(\mathbf{z}_{t}, \lambda_{t}\right)\)，进一步得到 \(\tilde{\boldsymbol{\epsilon}}_{\theta}\left(\mathbf{z}_{t}, \lambda_{t}, \mathbf{c}\right)\);</p>
  </li>
  <li>
    <p>step2：根据 \(\mathbf{z}_{t}\) 和  \(\tilde{\boldsymbol{\epsilon}}_{\theta}\left(\mathbf{z}_{t}, \lambda_{t}, \mathbf{c}\right)\) 得到 \(\hat{\mathbf{x}}_{\theta}\left(\mathbf{z}_{t}, \lambda_{t}, \mathbf{c}\right)=\left(\mathbf{z}_{t}-\sigma_{t} \tilde{\epsilon_{\theta}}\left(\mathbf{z}_{t}, \lambda_{t}, \mathbf{c}\right)\right) / \alpha_{t}\) ；</p>
  </li>
  <li>
    <p>step3：估计 \(\mathbf{z}_{s}=\alpha_{s} \hat{\mathbf{x}}_{\theta}\left(\mathbf{z}_{t}, \lambda_{t}, \mathbf{c}\right)+\sigma_{s}\tilde{\epsilon_{\theta}}\left(\mathbf{z}_{t}, \lambda_{t}, \mathbf{c}\right)\)</p>

    <p>​                      \(\mathbf{z}_{s}=\alpha_{s} \hat{\mathbf{x}}_{\theta}\left(\mathbf{z}_{t}, \lambda_{t}, \mathbf{c}\right)+\frac{\sigma_{s}}{\sigma_{t}}\left(\mathbf{z}_{t}-\alpha_{t} \hat{\mathbf{x}}_{\theta}\left(\mathbf{z}_{t}, \lambda_{t}, \mathbf{c}\right)\right)\)</p>
  </li>
</ul>

<p>式中， \(s&lt;t\) 在 1 到 0 之间取均匀分布序列，最先的采样来自对扩散过程的反转，注意到：</p>

\[q\left(\mathbf{z}_{s} \mid \mathbf{z}_{t}, \mathbf{x}\right)=\mathcal{N}\left(\mathbf{z}_{s} ; \tilde{\boldsymbol{\mu}}_{s \mid t}\left(\mathbf{z}_{t}, \mathbf{x}\right), \tilde{\sigma}_{s \mid t}^{2} \mathbf{I}\right)\]

<p>式中， \(\tilde{\boldsymbol{\mu}}_{s \mid t}\left(\mathbf{z}_{t}, \mathbf{x}\right)=e^{\lambda_{t}-\lambda_{s}}\left(\alpha_{s} / \alpha_{t}\right) \mathbf{z}_{t}+\left(1-e^{\lambda_{t}-\lambda_{s}}\right) \alpha_{s} \mathbf{x}\) ，并且  \(\tilde{\sigma}_{s \mid t}^{2}=\left(1-e^{\lambda_{t}-\lambda_{s}}\right) \sigma_{s}^{2}\), 遵循随机更新规则：</p>

\[\mathbf{z}_{s}=\tilde{\boldsymbol{\mu}}_{s \mid t}\left(\mathbf{z}_{t}, \hat{\mathbf{x}}_{\theta}\left(\mathbf{z}_{t}, \lambda_{t}, \mathbf{c}\right)\right)+\sqrt{\left(\tilde{\sigma}_{s \mid t}^{2}\right)^{1-\gamma}\left(\sigma_{t \mid s}^{2}\right)^{\gamma}} \boldsymbol{\epsilon}\]

<p>式中， \(\epsilon \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\), 其中 \(\gamma\) 控制采样的随机性。</p>

<h2 id="实施细节">实施细节</h2>

<p>Imagen 的提出的改进主要体现在：</p>

<ul>
  <li>引入新的动态阈值技术，这样采样器可以使用非常大的无分类器指导权重；</li>
  <li>在超分辨率模型中引入噪声增强，提高图像逼真度；</li>
  <li>引入一种新的高效 U-Net 架构，这种架构具有更高的计算效率、更高的内存效率和更快的收敛速度；</li>
</ul>

<h3 id="阈值技术">阈值技术</h3>

<p>增加 classifier-free guidance 的指导权重可以提高图像-文本的对齐，但会影响图像逼真度，产生高度饱和和不自然的图像。导致这个现象的原因是高指导权重引起训练测试不匹配：在每个采样步 \(t\)，\(x\) 的预测值 \(\hat x_0^t\) 必须与训练数据在同一范围内，即在 [−1, 1] 内。但使用高指导权重会使 \(x\) 预测值超出这些界限。这样就导致训练测试不匹配的情形，扩散模型在整个采样过程中会迭代应用自身输出，这样的采样过程会导致产生不自然的图像，有时甚至发散。</p>

<p>为解决上述训练测试不匹配问题，引入了阈值技术：</p>

<p><strong>静态阈值</strong>：对 \(x\) 的预测值逐元素裁剪到 [-1, 1] ，称为静态阈值。这样方法对于大引导权重的采样至关重要，并可以防止产生空白图片。尽管如此，随着指导权重的增加，静态阈值处理仍会使图像出现过饱和或细节少的问题，伪代码如下：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sample</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">)):</span>
        <span class="c1"># Forward pass to get x0_t from z_t.
</span>        <span class="n">x0_t</span> <span class="o">=</span> <span class="n">nn</span><span class="p">(</span><span class="n">z_t</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="c1"># Static thresholding.
</span>        <span class="n">x0_t</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">clip</span><span class="p">(</span><span class="n">x0_t</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="c1"># Sampler step.
</span>        <span class="n">z_tm1</span> <span class="o">=</span> <span class="n">sampler_step</span><span class="p">(</span><span class="n">x0_t</span><span class="p">,</span> <span class="n">z_t</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="n">z_t</span> <span class="o">=</span> <span class="n">z_tm1</span>
    <span class="k">return</span> <span class="n">x0_t</span>
</code></pre></div></div>

<p><strong>动态阈值</strong>：在每个采样步将 \(s\) 设置为 \(\hat x_0^t\) 中的某个百分位绝对像素值，如果 \(s &gt; 1\)，那么我们将 \(\hat x^t_0\) 调整到 [−s, s] 范围内，然后除以 \(s\)。动态阈值处理可以推动饱和像素（接近 -1 或 1) 向内收缩，从而主动防止像素在每一步产生饱和。这可显著提高图像的真实感，以及更好的图像文本对齐，尤其是在使用非常大的引导权重时，伪代码如下：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">p</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">)):</span>
        <span class="c1"># Forward pass to get x0_t from z_t.
</span>        <span class="n">x0_t</span> <span class="o">=</span> <span class="n">nn</span><span class="p">(</span><span class="n">z_t</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="c1"># Dynamic thresholding (ours).
</span>        <span class="n">s</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">x0_t</span><span class="p">),</span> <span class="n">p</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">x0_t</span><span class="p">.</span><span class="n">ndim</span><span class="p">)))</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="n">x0_t</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">clip</span><span class="p">(</span><span class="n">x0_t</span><span class="p">,</span> <span class="o">-</span><span class="n">s</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span> <span class="o">/</span> <span class="n">s</span>
        <span class="c1"># Sampler step.
</span>        <span class="n">z_tm1</span> <span class="o">=</span> <span class="n">sampler_step</span><span class="p">(</span><span class="n">x0_t</span><span class="p">,</span> <span class="n">z_t</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="n">z_t</span> <span class="o">=</span> <span class="n">z_tm1</span>
    <span class="k">return</span> <span class="n">x0_t</span>
</code></pre></div></div>

<figure align="center">
  <img src="/images/image-20220606111105610.png" style="zoom:100%" />
</figure>
<div>
    <center style="color:#C0C0C0;text-decoration:underline;font-size: 15px">
        图4: 使用 text-to-image 64 × 64 base 模型，比较了不同阈值方法的 CLIP 与 FID-10K score pareto frontiers。可以看出在较大的引导权重范围内，动态阈值方法可以取得明显更好的 CLIP 分数，以及相当甚至更好的 FID 分数
    </center>
    <br />
</div>

<figure align="center">
  <img src="/images/image-20220606111128575.png" style="zoom:100%" />
</figure>
<div>
    <center style="color:#C0C0C0;text-decoration:underline;font-size: 15px">
        图5: 不同阈值方法生成的图像
    </center>
    <br />
</div>

<h3 id="噪声增强">噪声增强</h3>

<p>Imagen 利用一个 64×64 base 模型，和两个文本条件超分辨率扩散模型，分别将生成的 64 × 64 图像上采样到 256 × 256 ，然后再上采样到 1024 × 1024 。 Imagen 对两个超分辨率模型都使用噪声调节增强 ，这对于生成高逼真度的图像至关重要。</p>

<p>训练时随机选择 aug_level ，推理时，选择比较不同的 aug_level 以找到最佳的样本质量。增强级别的范围 aug_level ∈ [0, 1] ，在实践中 aug_level 取 [0.1, 0.3] 的 会取得更好的效果，伪代码如下：</p>

<p>训练过程：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span>
    <span class="n">x_lr</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">x_hr</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">ndarray</span><span class="p">):</span>
    <span class="c1"># Add augmentation to the low-resolution image.
</span>    <span class="n">aug_level</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
    <span class="n">x_lr</span> <span class="o">=</span> <span class="n">apply_aug</span><span class="p">(</span><span class="n">x_lr</span><span class="p">,</span> <span class="n">aug_level</span><span class="p">)</span>
    <span class="c1"># Diffusion forward process.
</span>    <span class="n">t</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
    <span class="n">z_t</span> <span class="o">=</span> <span class="n">forward_process</span><span class="p">(</span><span class="n">x_hr</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    <span class="n">Optimize</span> <span class="n">loss</span><span class="p">(</span><span class="n">x_hr</span><span class="p">,</span> <span class="n">nn</span><span class="p">(</span><span class="n">z_t</span><span class="p">,</span> <span class="n">x_lr</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">aug_level</span><span class="p">))</span>
</code></pre></div></div>

<p>采样过程：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">aug_level</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">x_lr</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">ndarray</span><span class="p">):</span>
    <span class="c1"># Add augmentation to the low-resolution image.
</span>    <span class="n">x_lr</span> <span class="o">=</span> <span class="n">apply_aug</span><span class="p">(</span><span class="n">x_lr</span><span class="p">,</span> <span class="n">aug_level</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">)):</span>
        <span class="n">x_hr_t</span> <span class="o">=</span> <span class="n">nn</span><span class="p">(</span><span class="n">z_t</span><span class="p">,</span> <span class="n">x_lr</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">aug_level</span><span class="p">)</span>
        <span class="c1"># Sampler step.
</span>        <span class="n">z_tm1</span> <span class="o">=</span> <span class="n">sampler_step</span><span class="p">(</span><span class="n">x_hr_t</span><span class="p">,</span> <span class="n">z_t</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="n">z_t</span> <span class="o">=</span> <span class="n">z_tm1</span>
    <span class="k">return</span> <span class="n">x_hr_t</span>
</code></pre></div></div>

<figure align="center">
  <img src="/images/image-20220606115013668.png" style="zoom:100%" />
</figure>
<div>
    <center style="color:#C0C0C0;text-decoration:underline;font-size: 15px">
        图6: 64×64→256×256 超分辨率模型的 CLIP 与 FID-10K 的 pareto frontiers， aug_level 为在推理期间，在输入的低分辨率图像上实施的噪声增强级别，当 aug_level = 0 时，表示无噪声，对于所有指导权重值，都可以得到最佳 FID 分数，虽然较大的 aug_level 会影响 FID 分数，但它能产生更广的 CLIP 分数范围，意味着模型可以生成更多样的样本。
    </center>
    <br />
</div>

<figure align="center">
  <img src="/images/image-20220606114428973.png" style="zoom:100%" />
</figure>
<div>
    <center style="color:#C0C0C0;text-decoration:underline;font-size: 15px">
        图7: 使用大 aug_level 值和高指导权重，Imagen 在 64 × 64 图像基础上，输入不同的文本提示，生成不同变体。
    </center>
    <br />
</div>

<h3 id="网络架构">网络架构</h3>

<p><strong>Base 模型：</strong> 64 × 64 文本到图像 base 扩散模型使用 <sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup> 中的 U-Net 架构，调整点：（1）在整个文本嵌入序列的多个分辨率上添加交叉注意力；（2）在注意力和池化层的文本嵌入做层归一化。</p>

<p><strong>超分辨率模型：</strong>对于 64 × 64 → 256 × 256→ 1024 × 1024 的超分辨率模型，所使用的 U-Net 模型改编自 [40, 58]。为了提高记忆力效率、推理时间和收敛速度，对 U-Net 做了调整，称做 Efficient U-Net 。调整点：移除 self-attention 层，保留文本 cross-attention 层。在 64 × 64 → 256 × 256 的超分辨率任务上，U-Net 和 Efficient U-Net 的性能对比如下图，可以看出 Efficient U-Net 的收敛速度明显快于 U-Net, 并且可以获得更好的性能，此外 Efficient U-Net 在采样速度上也可以加快 ×2 − 3 倍。</p>

<figure align="center">
  <img src="/images/image-20220606110602246.png" style="zoom:100%" />
</figure>
<div>
    <center style="color:#C0C0C0;text-decoration:underline;font-size: 15px">
        图8: U-Net 与 Efficient U-Net 的收敛速度比较
    </center>
    <br />
</div>
<figure align="center">
  <img src="/images/image-20220606110057800.png" style="zoom:60%" />
</figure>
<div>
    <center style="color:#C0C0C0;text-decoration:underline;font-size: 15px">
        图9: Efficient U-Net architecture for 642 → 2562
    </center>
    <br />
</div>

<p>ResNetBlock 在 DBlock 和 UBlock 都会被使用，ResNetBlock 的 超参数是 channels 数。</p>

<figure align="center">
  <img src="/images/image-20220606105557305.png" style="zoom:40%" />
</figure>
<div>
    <center style="color:#C0C0C0;text-decoration:underline;font-size: 15px">
        图10: Efficient U-Net ResNetBlock
    </center>
    <br />
</div>

<p>DBlock 的超参数是：下采样步长 stride: Optional [Tuple[int, int]]，每个 DBlock 的 ResNetBlock 数量 numResNetBlocksPerBlock：int，通道数 channels：int， 虚线块是可选的，例如，并非每个 DBlock 都需要下采样或需要自注意。</p>

<figure align="center">
  <img src="/images/image-20220606105820690.png" style="zoom:100%" />
</figure>
<div>
    <center style="color:#C0C0C0;text-decoration:underline;font-size: 15px">
        图11: Efficient UNet DBlock
    </center>
    <br />
</div>

<p>UBlock 的超参数是：上采样步长 stride: Optional[Tuple[int, int]]，每个 DBlock 的 ResNetBlock 数量 numResNetBlocksPerBlock：int，通道数 channels：int， 虚线块是可选的。</p>

<figure align="center">
  <img src="/images/image-20220606105848696.png" style="zoom:100%" />
</figure>
<div>
    <center style="color:#C0C0C0;text-decoration:underline;font-size: 15px">
        图12: Efficient U-Net UBlock
    </center>
    <br />
</div>

<p><strong>文本嵌入和模型尺寸：</strong>图 A.13a 比较了对文本嵌入进行平均池化、注意力池化和交叉注意力的性能，使用交叉注意力可以取得更好的性能。图 A.13b 绘制了 64 × 64  各种 U-Net 模型大小的 CLIP-FID 得分权衡曲线。 随着模型的增大，可以获得更好的权衡曲线；另外，增大文本编码器模型相比 U-Net 模型，会产生更好的结果。</p>

<figure align="center">
  <img src="/images/image-20220606121529918.png" style="zoom:100%" />
</figure>
<div>
    <center style="color:#C0C0C0;text-decoration:underline;font-size: 15px">
        图13: 不同文本编码器条件模式和U-Net 模型大小性能比较
    </center>
    <br />
</div>

<h2 id="参考">参考</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1">
      <p><a href="https://imagen.research.google/">Imagen: Text-to-Image Diffusion Models (research.google)</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>OpenAI DALL-E blog: https://openai.com/blog/dall-e/ <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>Alex Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. arXiv preprint arXiv:2102.09672, 2021. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

      <footer class="entry-meta">
        <span class="entry-tags"><a href="http://localhost:4000/tags/#imagen" title="Pages tagged imagen" class="tag"><span class="term">imagen</span></a><a href="http://localhost:4000/tags/#diffusion" title="Pages tagged diffusion" class="tag"><span class="term">diffusion</span></a><a href="http://localhost:4000/tags/#score function" title="Pages tagged score function" class="tag"><span class="term">score function</span></a><a href="http://localhost:4000/tags/#guidance" title="Pages tagged guidance" class="tag"><span class="term">guidance</span></a><a href="http://localhost:4000/tags/#deep learning" title="Pages tagged deep learning" class="tag"><span class="term">deep learning</span></a><a href="http://localhost:4000/tags/#generative models" title="Pages tagged generative models" class="tag"><span class="term">generative models</span></a></span>
        
        
      </footer>
    </div><!-- /.entry-content -->

    <div class="read-more">
  
    <div class="read-more-header">
      <a href="http://localhost:4000" class="read-more-btn">Read More</a>
    </div><!-- /.read-more-header -->
    <div class="read-more-content">
      <h3><a href="http://localhost:4000/2025/04/01/vla.html" title="VLM/VLA相关调研">VLM/VLA相关调研</a></h3>
      <p>一些VLM和VLA模型的简单介绍 <a href="http://localhost:4000/2025/04/01/vla.html">Continue reading</a></p>
    </div><!-- /.read-more-content -->
  
  <div class="read-more-list">
    
  </div><!-- /.read-more-list -->
</div><!-- /.read-more -->
  </article>
</div><!-- /#main -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="http://localhost:4000/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="http://localhost:4000/assets/js/scripts.min.js"></script>


<!-- Asynchronous Google Analytics snippet -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'G-2WZMKC0BR6', 'auto');  
  ga('require', 'linkid', 'linkid.js');
  ga('send', 'pageview');
</script>


<!-- Link Gitalk 的支持文件  -->
<link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<script src="https://unpkg.com/gitalk@latest/dist/gitalk.min.js"></script> 
<div id="gitalk-container"></div>     <script type="text/javascript">
    var gitalk = new Gitalk({

    // gitalk的主要参数
        clientID: '7c854c0f32253f034752',
        clientSecret: '090c39c28b126c78184f1b2fd4d12f30d18d196f',
        repo: 'sunlin-ai.github.io',
        owner: 'sunlin-ai',
        admin: ['sunlin-ai'],
        id:decodeURI(window.location.pathname),

    });
    gitalk.render('gitalk-container');
</script> 
<!-- Gitalk end -->

<div class="footer-wrapper">
  <footer role="contentinfo">
    <span>&copy; 2025 SI-YU. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> using the <a href="https://mademistakes.com/work/hpstr-jekyll-theme/" rel="nofollow">HPSTR Theme</a>.</span>
  </footer>
</div><!-- /.footer-wrapper -->

	        

</body>
</html>
